{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning goal\n",
    "In this notebook you will learn how to retrieve Google Trend's search interest data at scale and in a reliable way. This comes in handy for queries with thousands of keywords to build a dataset.  \n",
    "\n",
    "# Problem\n",
    "\n",
    "Using Google Trends to see search interest for a keyword of the last five years works well for a small number queries. With more queries, Google's server will deny their service, returning \"too many requests\" errors, rate limit exceedance, or blacklist your IP. A suggested workaround is to use proxies. However, this often causes other errors later on when they get out of service or have other issues which makes them inaccessible. You would have to reconfigure them once in a while when revisiting your code. \n",
    "\n",
    "To alleviate this problem, I rely on timeouts. Sufficiently long intervals between queries minimze the risk of request errors. In addition to this, I provide a fallback procedure in case of errors. It stores previously collected data and initiates another attempt. The procedure might take longer though especially for thousands of queries. But the computer can work, while you sleep. enjoy life or make big plans for the next project that involves Google Trends. Because you will know how to work it after reading this article.\n",
    "\n",
    "# Challenges and limits of Google Trends\n",
    "\n",
    "1. Denial of service, exceeding rate limits, being blacklisted\n",
    "3. Maximum 5 keywords per query\n",
    "2. Relative measures and scalability (solved by @Carrie Fowle\n",
    "in https://towardsdatascience.com/using-google-trends-at-scale-1c8b902b6bfa)\n",
    "\n",
    "\n",
    "While we focus on the first issue, there will be workarounds included for all three along the way.   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implementation plan\n",
    "\n",
    "\n",
    "\n",
    "1. look through code and judge where to refactor\n",
    "    1. rely on helper functions\n",
    "    2. define keyword_constructor()\n",
    "1. set Gtrends query into a function\n",
    "    2. save dataset along the way\n",
    "    2. simulate error and retry at last idx\n",
    "    3. if retry unsuccessful, increase timeout until no exception \n",
    "        1. abort with message if unsuccessful after 10 increases \"wait for a bit and define a longer timeout\"\n",
    "\n",
    "    \n",
    "## Data sources\n",
    "\n",
    "- search interest: Google Trends\n",
    "- search interest+: Google autocompletion\n",
    "- news coverage: Google News\n",
    "- ESG scores, financial data, sector: Yahoo!finance\n",
    "\n",
    "\n",
    "\n",
    "### NEXT\n",
    "\n",
    "* sentiment analysis with Tensorflow\n",
    "* multiple outcomes\n",
    "* helper functions into directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work\n",
    "\n",
    "**Google Trends data**\n",
    "\n",
    "* include time dimension\n",
    "* Scale of search interest has to be comparable and scaled within each firm: include firm name in every batch \n",
    "\n",
    "**Advanced input features**\n",
    "\n",
    "* Google autocompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6\n",
      "Pandas version 1.0.3\n"
     ]
    }
   ],
   "source": [
    "# data preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "from math import ceil\n",
    "from math import exp\n",
    "import os\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data collection \n",
    "from yahooquery import Ticker\n",
    "\n",
    "py_version = !python --version\n",
    "print(py_version[0])\n",
    "print(\"Pandas version\",pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inlcude into helper functions\n",
    "# KEYWORD GENERATOR HELPERS \n",
    "def regex_strip_legalname(raw_names):\n",
    "    \"\"\"Removes legal entity, technical description or firm type from firm name\n",
    "    \n",
    "    Input\n",
    "        raw_names: list of strings with firm names\n",
    "        \n",
    "    Return\n",
    "        list of strings: firm names without legal description \n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: \n",
    "    \n",
    "    pattern = r\"(?!Incyte)(\\s)*(Enterprise|Worldwide|Int\\'l|LLC|Inc|Corp\\w*|\\(?Class \\w+\\)?|Group|Company|\\WCo(\\s|\\.)|plc|Ltd|Int'l\\.|Holdings)\\.?\\W?\"\n",
    "    stripped_names = [re.sub(pattern,'', n) for n in raw_names]\n",
    "    \n",
    "    return stripped_names\n",
    "\n",
    "def batch(lst, n=5):\n",
    "    \"\"\"Yield successive n-sized chunks from list lst\n",
    "    \n",
    "    adapted from https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \n",
    "    Input\n",
    "        lst: list \n",
    "        n: selected batch size\n",
    "        \n",
    "    Return \n",
    "        List: lst divided into batches of len(lst)/n lists\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def flatten_list(nested_list):\n",
    "    \"\"\"Flattens nested list\"\"\"\n",
    "    return [element for sublist in nested_list for element in sublist]\n",
    "\n",
    "def list_remove_duplicates(l):\n",
    "    \"\"\"Removes duplicates from list elements whilst preserving element order\n",
    "    adapted from \n",
    "    https://stackoverflow.com/questions/480214/how-do-you-remove-duplicates-from-a-list-whilst-preserving-order\n",
    "    \n",
    "    Input\n",
    "        list with string elements\n",
    "    \n",
    "    Return \n",
    "        Sorted list without duplicates\n",
    "    \n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in l if not (x in seen or seen_add(x))]\n",
    "    \n",
    "def make_x_y_csv(x, y, filename, data_dir):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param y: Data labels\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "    \n",
    "    # create dir if nonexistent\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # merge df\n",
    "    y = pd.DataFrame(y)\n",
    "    x = pd.DataFrame(x)\n",
    "    \n",
    "    # export to csv\n",
    "    pd.concat([y, x], axis=1).to_csv(os.path.join(data_dir, filename), \n",
    "                                     header=False, \n",
    "                                     index=False)\n",
    "    \n",
    "    # nothing is returned, but a print statement indicates that the function has run\n",
    "    print('Path created: '+str(data_dir)+'/'+str(filename))\n",
    "    \n",
    "def make_csv(x, filename, data_dir, append=False, header=False, index=False):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "    \n",
    "    # create dir if nonexistent\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # make sure its a df\n",
    "    x = pd.DataFrame(x)\n",
    "    \n",
    "    # export to csv\n",
    "    if not append:\n",
    "        x.to_csv(os.path.join(data_dir, filename), \n",
    "                                     header=header, \n",
    "                                     index=index)\n",
    "    else:\n",
    "        x.to_csv(os.path.join(data_dir, filename),\n",
    "                                     mode = 'a',\n",
    "                                     header=header, \n",
    "                                     index=index)        \n",
    "    \n",
    "    # nothing is returned, but a print statement indicates that the function has run\n",
    "    print('Path created: '+str(data_dir)+'/'+str(filename))\n",
    "    \n",
    "from time import strftime\n",
    "\n",
    "def timestamp_now():\n",
    "    \"\"\"Create timestamp string in format: yyyy/mm/dd-hh/mm/ss\n",
    "    Input\n",
    "        None\n",
    "        \n",
    "    Return\n",
    "        String: Timestamp for current time\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    timestr = strftime(\"%Y%m%d-%H%M%S\")\n",
    "    timestamp = '{}'.format(timestr)  \n",
    "    \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineer search keywords from firm names and topic\n",
    "\n",
    "**TODO:** def construct_search_keywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firm names of S&P500\n",
    "### Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve S&P 500 listings from Wikipedia\n",
    "table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "df_sp500 = table[0]\n",
    "\n",
    "## retrieve firm information from table\n",
    "# ticker\n",
    "ticker = list(df_sp500.Symbol)\n",
    "# sector\n",
    "sector = df_sp500.loc[:,'GICS Sector']\n",
    "# firm names\n",
    "firm_names_raw = list(df_sp500.Security)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocces: Remove legal suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/firm_names.csv\n"
     ]
    }
   ],
   "source": [
    "## Regex job\n",
    "# remove legal taxonomy and firm type\n",
    "# drop duplicates\n",
    "firm_names = list_remove_duplicates(\n",
    "    regex_strip_legalname(firm_names_raw)\n",
    ")\n",
    "\n",
    "make_csv(firm_names, 'firm_names.csv', 'data', append=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/topics.csv\n"
     ]
    }
   ],
   "source": [
    "# esg keywords (negative exclusion criteria)\n",
    "topics = ['scandal', 'greenwashing', 'corruption', 'fraud', 'bribe', 'tax', 'forced', 'harassment', 'violation', \n",
    "          'rights', 'conflict', 'weapons', 'arms', 'pollution', 'CO2', 'emission', 'fossil',\n",
    "          'inequality', 'discrimination', 'sexism', 'racist', 'intransparent', 'data', 'lawsuit', \n",
    "          'unfair', 'bad', 'problem', 'hate', 'issues', 'controversial', \n",
    "         'green', 'sustainable', 'positive', 'best', 'good', 'social', 'charity', 'ethical', 'renewable', 'neutral']\n",
    "\n",
    "# store lists as csv for retrieval\n",
    "make_csv(topics, 'topics.csv', 'data', append=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> Subset for testing purposes\n",
      "Generated 1200 keywords for 30 firms and 40 topics each\n",
      "Resulting in 240 queries with 5 keywords each (=batch)\n",
      "\n",
      "Example keyword batch:\n",
      "['scandal 3M', 'greenwashing 3M', 'corruption 3M', 'fraud 3M', 'bribe 3M']\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# DEFINE PARAMETERS \n",
    "n_firms = 30\n",
    "batch_size = 5\n",
    "n_keywords = int(n_firms*len(topics))\n",
    "n_query = int(n_keywords/batch_size)\n",
    "n_topics = len(topics)\n",
    "############################\n",
    "\n",
    "\n",
    "# create search keywords as pairwise combintations of firm names + topics\n",
    "search_keywords = [[j+' '+i for j in topics] for i in firm_names]\n",
    "\n",
    "\n",
    "# Subset for test purposes\n",
    "print(\">>>>>>> Subset for testing purposes\")\n",
    "keywords_sample = search_keywords[:n_firms]\n",
    "print(\"Generated {} keywords for {} firms and {} topics each\".format(n_keywords,n_firms,n_topics))\n",
    "print(\"Resulting in {} queries with {} keywords each (=batch)\".format(n_query, batch_size))\n",
    "\n",
    "## generate keyword batches (= query)\n",
    "# flatten list\n",
    "keyword_batches = flatten_list([list(batch(keywords_sample[i])) for i in range(n_firms)])\n",
    "\n",
    "print(\"\\nExample keyword batch:\\n{}\".format(keyword_batches[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_query_results(df_result, keywords, query_return_length=259):\n",
    "    \"\"\"Process query results: \n",
    "            (i) check for empty response --> create df with 0s if empty\n",
    "            (ii) drop isPartial rows and column\n",
    "            (iii) transpose dataframe to wide format (keywords//search interest)\n",
    "    \n",
    "    Input\n",
    "        df: dataframe containing query result (could be empty)\n",
    "        filename: name of temporary file\n",
    "        query_return_length: 259 is normal return length of query result \n",
    "                            (without \"isPartial\" column)\n",
    "        \n",
    "    Return\n",
    "        Dataframe: contains query results in long format \n",
    "        (rows: keywords, columns: search interest over time)\n",
    "    \"\"\"\n",
    "    # (i) non-empty df\n",
    "    if df_result.shape[0] != 0:\n",
    "        # (ii) drop rows with isPartial == True and drop column\n",
    "        df_result = df_result[df_result.isPartial == 'False'].drop(columns='isPartial')\n",
    "        df_result.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        # (iii) transpose df, to have a wide format (keywords//search interest)\n",
    "        return df_result.T\n",
    "\n",
    "    # empty df: no search result for any keyword\n",
    "    else:        \n",
    "        # create df containing 0s\n",
    "        df_result = pd.DataFrame(np.zeros((query_return_length,batch_size)), columns=keywords)\n",
    "\n",
    "        # (iii) transpose df, to have a wide format (keywords//search interest)\n",
    "        return df_result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "import time # for sleep and timestamp\n",
    "\n",
    "def google_query(batched_keywords, sec_sleep=30):\n",
    "    \"\"\"Get Google trends data in a reliable way\n",
    "        if server does not respond, store results so far and retry with increased timeout\n",
    "    \n",
    "    Input\n",
    "        batched_keywords: list of keywords with chunks of five\n",
    "        temp_data: name of temporary file in ./data/ directory (defined in make_csv())\n",
    "\n",
    "    Return\n",
    "        None: stores query results as .csv in ./data/ \n",
    "    \"\"\"\n",
    "    # initialize pytrends\n",
    "    pt = TrendReq(hl='en-US', retries=3)\n",
    "    \n",
    "    # empty list to store dataframes\n",
    "    df_list = []\n",
    "    \n",
    "    ## iterate over keyword batches to obtain query results\n",
    "    for i, batch in enumerate(batched_keywords):\n",
    "    \n",
    "        # make query\n",
    "        try:\n",
    "            # pass keywords to pytrends API \n",
    "            pt.build_payload(kw_list=batch) \n",
    "\n",
    "            # store results from query in df and append to df_list\n",
    "            df_query_result = pt.interest_over_time()\n",
    "            \n",
    "            # check if empty and transpose to long format\n",
    "            df_query_result_long = handle_query_results(df_query_result, batch)\n",
    "            df_list.append(df_query_result_long)\n",
    "            \n",
    "            # wait (timeout)\n",
    "            time.sleep(sec_sleep)\n",
    "        \n",
    "        # error handling\n",
    "        except Exception as e:\n",
    "            print(\"Error {} for batch {}\".format(e, i))\n",
    "            \n",
    "            # merge results fetched so far\n",
    "            df_query_result = pd.concat(df_list)\n",
    "            \n",
    "            # store results in csv, indicating last successful batch (i-1) and timestamp\n",
    "            timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            df_filename = '{}_googletrends_batch_{}.csv'.format(timestr, i-1)            \n",
    "            make_csv(df_query_result, df_filename, 'data', index=True, header=True)\n",
    "            print(\"Store results in\", df_filename)\n",
    "            \n",
    "            # recursively call function with keyword_batches starting from i\n",
    "            # and an increased timeout by 10 seconds\n",
    "            sec_sleep += 10\n",
    "            print(\"Increased sec_sleep to {}\".format(sec_sleep))\n",
    "            google_query(batched_keywords[i:], sec_sleep+10)\n",
    "    \n",
    "    \n",
    "    ## finally store in csv\n",
    "    # merge query results\n",
    "    df_all_results = pd.concat(df_list)\n",
    "    # store results in csv, indicating last batch (i) and timestamp\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    df_filename = '{}_googletrends_batch_{}.csv'.format(timestr, i)  \n",
    "    \n",
    "    make_csv(df_all_results, df_filename, 'data', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['racist Advanced Micro Devices',\n",
       "  'intransparent Advanced Micro Devices',\n",
       "  'data Advanced Micro Devices',\n",
       "  'lawsuit Advanced Micro Devices',\n",
       "  'unfair Advanced Micro Devices'],\n",
       " ['social Albemarle',\n",
       "  'charity Albemarle',\n",
       "  'ethical Albemarle',\n",
       "  'renewable Albemarle',\n",
       "  'neutral Albemarle'],\n",
       " ['tax Alexandria Real Estate Equities',\n",
       "  'forced Alexandria Real Estate Equities',\n",
       "  'harassment Alexandria Real Estate Equities',\n",
       "  'violation Alexandria Real Estate Equities',\n",
       "  'rights Alexandria Real Estate Equities'],\n",
       " ['tax Altria',\n",
       "  'forced Altria',\n",
       "  'harassment Altria',\n",
       "  'violation Altria',\n",
       "  'rights Altria'],\n",
       " ['scandal Amcor',\n",
       "  'greenwashing Amcor',\n",
       "  'corruption Amcor',\n",
       "  'fraud Amcor',\n",
       "  'bribe Amcor'],\n",
       " ['green American Electric Power',\n",
       "  'sustainable American Electric Power',\n",
       "  'positive American Electric Power',\n",
       "  'best American Electric Power',\n",
       "  'good American Electric Power']]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[keyword_batches[i] for i in [60,127,129,185,200,230]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/20200818-234505_googletrends_batch_5.csv\n"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "test_kw = [keyword_batches[i] for i in [60,127,129,185,200,230]]\n",
    "google_query(test_kw, sec_sleep=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing: Google Trends\n",
    "\n",
    "| firm | scandal | emissions | ... | kw\n",
    "| --- | --- | --- | --- | ---\n",
    "| Apple | 30 | 12 | ... | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>racist Advanced Micro Devices</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intransparent Advanced Micro Devices</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data Advanced Micro Devices</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lawsuit Advanced Micro Devices</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfair Advanced Micro Devices</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      1    2    3    4    5    6    7    8    \\\n",
       "0                                                                              \n",
       "racist Advanced Micro Devices         0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "intransparent Advanced Micro Devices  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "data Advanced Micro Devices           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "lawsuit Advanced Micro Devices        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "unfair Advanced Micro Devices         0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                                      9    10   ...  251  252  253  254  255  \\\n",
       "0                                               ...                            \n",
       "racist Advanced Micro Devices         0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "intransparent Advanced Micro Devices  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "data Advanced Micro Devices           0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "lawsuit Advanced Micro Devices        0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "unfair Advanced Micro Devices         0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                                      256  257  258  259  260  \n",
       "0                                                              \n",
       "racist Advanced Micro Devices         0.0  0.0  0.0  0.0  NaN  \n",
       "intransparent Advanced Micro Devices  0.0  0.0  0.0  0.0  NaN  \n",
       "data Advanced Micro Devices           0.0  0.0  0.0  0.0  NaN  \n",
       "lawsuit Advanced Micro Devices        0.0  0.0  0.0  0.0  NaN  \n",
       "unfair Advanced Micro Devices         0.0  0.0  0.0  0.0  NaN  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read scraped Google Trends data\n",
    "df_gtrends = pd.read_csv(\"./data/20200818-234505_googletrends_batch_5.csv\", index_col=0, header=None)\n",
    "df_gtrends.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>bribe</th>\n",
       "      <th>charity</th>\n",
       "      <th>corruption</th>\n",
       "      <th>data</th>\n",
       "      <th>ethical</th>\n",
       "      <th>forced</th>\n",
       "      <th>fraud</th>\n",
       "      <th>good</th>\n",
       "      <th>green</th>\n",
       "      <th>...</th>\n",
       "      <th>positive</th>\n",
       "      <th>racist</th>\n",
       "      <th>renewable</th>\n",
       "      <th>rights</th>\n",
       "      <th>scandal</th>\n",
       "      <th>social</th>\n",
       "      <th>sustainable</th>\n",
       "      <th>tax</th>\n",
       "      <th>unfair</th>\n",
       "      <th>violation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Advanced Micro Devices</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albemarle</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.519231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alexandria Real Estate Equities</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altria</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.669231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amcor</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 best  bribe  charity  corruption  data  \\\n",
       "firm                                                                      \n",
       "Advanced Micro Devices            NaN    NaN      NaN         NaN   0.0   \n",
       "Albemarle                         NaN    NaN      0.0         NaN   NaN   \n",
       "Alexandria Real Estate Equities   NaN    NaN      NaN         NaN   NaN   \n",
       "Altria                            NaN    NaN      NaN         NaN   NaN   \n",
       "Amcor                             NaN    0.0      NaN         0.0   NaN   \n",
       "\n",
       "                                 ethical  forced  fraud  good  green  ...  \\\n",
       "firm                                                                  ...   \n",
       "Advanced Micro Devices               NaN     NaN    NaN   NaN    NaN  ...   \n",
       "Albemarle                            0.0     NaN    NaN   NaN    NaN  ...   \n",
       "Alexandria Real Estate Equities      NaN     0.0    NaN   NaN    NaN  ...   \n",
       "Altria                               NaN     0.0    NaN   NaN    NaN  ...   \n",
       "Amcor                                NaN     NaN    0.0   NaN    NaN  ...   \n",
       "\n",
       "                                 positive  racist  renewable  rights  scandal  \\\n",
       "firm                                                                            \n",
       "Advanced Micro Devices                NaN     0.0        NaN     NaN      NaN   \n",
       "Albemarle                             NaN     NaN        0.0     NaN      NaN   \n",
       "Alexandria Real Estate Equities       NaN     NaN        NaN     0.0      NaN   \n",
       "Altria                                NaN     NaN        NaN     0.0      NaN   \n",
       "Amcor                                 NaN     NaN        NaN     NaN      0.0   \n",
       "\n",
       "                                    social  sustainable        tax  unfair  \\\n",
       "firm                                                                         \n",
       "Advanced Micro Devices                 NaN          NaN        NaN     0.0   \n",
       "Albemarle                        37.519231          NaN        NaN     NaN   \n",
       "Alexandria Real Estate Equities        NaN          NaN   0.000000     NaN   \n",
       "Altria                                 NaN          NaN  18.669231     NaN   \n",
       "Amcor                                  NaN          NaN        NaN     NaN   \n",
       "\n",
       "                                 violation  \n",
       "firm                                        \n",
       "Advanced Micro Devices                 NaN  \n",
       "Albemarle                              NaN  \n",
       "Alexandria Real Estate Equities        0.0  \n",
       "Altria                                 0.0  \n",
       "Amcor                                  NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reverse engineer firm names into a column and topics as index\n",
    "firm_list_df = []\n",
    "topic_list_df = []\n",
    "for keyword in df_gtrends.index:\n",
    "    firm = [x for x in firm_names if x in keyword][0]\n",
    "    topic = keyword.replace(firm, \"\").split()[0] # take only first word of topic\n",
    "    \n",
    "    # map: keyword -> firm\n",
    "    firm_list_df.append(firm)\n",
    "    \n",
    "    # map: keyword -> topic\n",
    "    topic_list_df.append(topic)\n",
    "\n",
    "# create column for firm names\n",
    "df_gtrends['firm'] = firm_list_df\n",
    "# \n",
    "df_gtrends.index = topic_list_df\n",
    "\n",
    "# pivot table tranform 1 row = 1 firm\n",
    "df_nested = df_gtrends.pivot_table(values=[i for i in range(1,df_gtrends.shape[1])], index=['firm', df_gtrends.index])\n",
    "# calculate overall average\n",
    "df_trends_mean = pd.DataFrame(df_nested.mean(axis=1)).unstack()\n",
    "df_trends_mean.columns = df_trends_mean.columns.droplevel()\n",
    "df_trends_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def add_years(d, years):\n",
    "    \"\"\"Add/subtract a year from today's date \n",
    "    Return the same calendar date (month and day) in the\n",
    "    destination year, if it exists, otherwise use the following day\n",
    "    (e.g. changing February 29 to March 1).\n",
    "    \n",
    "    Source: https://stackoverflow.com/a/15743908\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return d.replace(year = d.year + years)\n",
    "    except ValueError:\n",
    "        return d + (date(d.year + years, 1, 1) - date(d.year, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from GoogleNews import GoogleNews\n",
    "\n",
    "def get_news(keyword, until_page=10, keep_columns=['title', 'date', 'desc']):\n",
    "    \"\"\"Retrieve news for keyword for the first specified number of result pages\n",
    "        within the period until 1 year ago\n",
    "        \n",
    "    Input\n",
    "        keyword to look up news for\n",
    "    \n",
    "    Return\n",
    "        dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    ## define 1 year timespan with datestrings \n",
    "    # today's date\n",
    "    date_today = date.today().strftime(\"%m/%d/%Y\")\n",
    "    # date 1 year ago\n",
    "    date_1year_ago = add_years(date.today(), -1).strftime(\"%m/%d/%Y\")\n",
    "\n",
    "    ## Google news query\n",
    "    # init googlenews object\n",
    "    googlenews=GoogleNews(lang='en', start=date_1year_ago, end=date_today)    \n",
    "    \n",
    "    # retrieve search news for keyword\n",
    "    googlenews.search(keyword)\n",
    "    \n",
    "    # get results for each page \n",
    "    for p in range(until_page):\n",
    "        googlenews.getpage(p)\n",
    "    \n",
    "    # store results in df\n",
    "    result = pd.DataFrame(googlenews.result())\n",
    "    \n",
    "    ## process result data\n",
    "    # drop duplicates\n",
    "    result.drop_duplicates(inplace=True)\n",
    "    # keep specified columns\n",
    "    result = result[keep_columns]\n",
    "    \n",
    "    # add column with keyword\n",
    "    result['keyword'] = keyword\n",
    "    \n",
    "    # clear google news cache\n",
    "    googlenews.clear()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \"None of [Index(['title', 'date', 'desc'], dtype='object')] are in the [columns]\"\n",
      "Firm:Estée Lauder Companies\n",
      "Error: \"None of [Index(['title', 'date', 'desc'], dtype='object')] are in the [columns]\"\n",
      "Firm:Hartford Financial Svc.Gp.\n",
      "Error: \"None of [Index(['title', 'date', 'desc'], dtype='object')] are in the [columns]\"\n",
      "Firm:Iron Mountainorporated\n",
      "Error: \"None of [Index(['title', 'date', 'desc'], dtype='object')] are in the [columns]\"\n",
      "Firm:Realtyome\n"
     ]
    }
   ],
   "source": [
    "# get news for all firms\n",
    "df_list = []\n",
    "firm_error = []\n",
    "\n",
    "for firm in firm_names:\n",
    "    try:\n",
    "        df_news = get_news(firm, until_page=10)\n",
    "        df_list.append(df_news)\n",
    "    except Exception as e: \n",
    "        print(\"Error: {}\\nFirm:{}\".format(e, firm))    \n",
    "        firm_error.append(firm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error for Estée Lauder Companies, Hartford Financial Svc.Gp., Mountainorporated, Realtyome\n",
    "\n",
    "# merge df\n",
    "df_news_firms = pd.concat(df_list, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/Googlenews_large.csv\n"
     ]
    }
   ],
   "source": [
    "# store as csv\n",
    "make_csv(df_news_firms, timestamp_now()+'_Googlenews_large.csv', 'data', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "################# LEGACY NOT NEEDED ANYMORE \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pytrends.request import TrendReq \n",
    "from time import sleep \n",
    "\n",
    "# PYTREND HELPERS\n",
    "def pytrends_sleep_init(seconds, temp_csv):\n",
    "    \"\"\"Timeout for certain seconds and re-initialize pytrends\n",
    "    \n",
    "    Input\n",
    "        seconds: int with seconds for timeout\n",
    "        \n",
    "    Return\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "    # print(\"TIMEOUT for {} sec.\".format(seconds))\n",
    "    sleep(seconds)\n",
    "    \n",
    "    # initialize pytrends\n",
    "    pt = TrendReq()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "################# LEGACY NOT NEEDED ANYMORE replaced by google_query()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## retrieve Google trends across time\n",
    "\n",
    "# initialize pytrends\n",
    "pt = TrendReq()\n",
    "\n",
    "# store DFs for later concat\n",
    "df_list = []\n",
    "index_batch_error = []\n",
    "\n",
    "# create csv to store intermediate results\n",
    "make_csv(pd.DataFrame(), 'googletrends.csv', data_dir='data')\n",
    "\n",
    "for i, batch in enumerate(keyword_batches):\n",
    "    \n",
    "    ## retrieve interest over time\n",
    "    try:\n",
    "        # init pytrends and wait (timeout)\n",
    "        pytrends_sleep_init(sec_sleep)\n",
    "        \n",
    "        # pass keywords to pytrends API\n",
    "        pt.build_payload(kw_list=batch) \n",
    "        \n",
    "        print(\"Payload build for {}. batch\".format(i))\n",
    "        df_search_result = pt.interest_over_time()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Query {} of {}\".format(i, n_query))\n",
    "        # store index at which error occurred\n",
    "        index_batch_error.append(i)\n",
    "        \n",
    "        # re-init pytrends and wait (sleep/timeout)\n",
    "        pytrends_sleep_init(sec_sleep)\n",
    "        \n",
    "        # retry\n",
    "        print(\"RETRY for {}. batch\".format(i))\n",
    "        pt.build_payload(kw_list=batch) \n",
    "        df_search_result = pt.interest_over_time()\n",
    "        \n",
    "    ## store query results\n",
    "    # check for non-empty df\n",
    "    if df_search_result.shape[0] != 0:\n",
    "        \n",
    "        # reset index for consistency (to call pd.concat later with empty dfs)\n",
    "        df_search_result.reset_index(inplace=True)\n",
    "        df_list.append(df_search_result)\n",
    "        \n",
    "    # no search result for any keyword\n",
    "    else:        \n",
    "        # create df containing 0s\n",
    "        df_search_result = pd.DataFrame(np.zeros((261,batch_size)), columns=batch)\n",
    "        df_list.append(df_search_result)\n",
    "        \n",
    "    make_csv(df_search_result, filename='googletrends.csv', data_dir='data',\n",
    "             append=True,\n",
    "            header=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# combine query results to df\n",
    "drop_cols = ['isPartial', 'date']\n",
    "\n",
    "# index df\n",
    "df_clean_list = []\n",
    "for i,x in enumerate(range(0,len(df_list),6)):\n",
    "\n",
    "    map_colnames = dict(zip(search_keywords[i+272], list(topics)))\n",
    "    \n",
    "    ## create firm-level df\n",
    "    # df with isPartial and date columns --> drop columns\n",
    "    try:\n",
    "        df_firm = pd.concat(df_list[x:x+6], axis=1).drop(columns=drop_cols)\n",
    "\n",
    "        # rename columns\n",
    "        df_firm.rename(columns=map_colnames, inplace=True)\n",
    "        \n",
    "        # add firm column\n",
    "        df_firm['firm'] = firm_names[i+272]\n",
    "        \n",
    "        df_clean_list.append(df_firm)\n",
    "        \n",
    "    except:\n",
    "        df_firm = pd.concat(df_list[x:x+6], axis=1).rename(columns=map_colnames)\n",
    "        # rename columns\n",
    "        df_firm.rename(columns=map_colnames, inplace=True)\n",
    "        # add firm \n",
    "        df_firm['firm'] = firm_names[i]\n",
    "\n",
    "        df_clean_list.append(df_firm)\n",
    "\n",
    "# df (long format) with time dimension      \n",
    "df_time = pd.concat(df_clean_list)\n",
    "\n",
    "# Store query results so far\n",
    "print('Index batch error:',index_batch_error)\n",
    "\n",
    "# get timestamp\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df_filename = 'df_time_{}_idxbatch_{}.csv'.format(timestr, 1633)\n",
    "print(df_filename)\n",
    "\n",
    "# Store df_time\n",
    "make_csv(df_time, filename=df_filename, data_dir='data', append=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
