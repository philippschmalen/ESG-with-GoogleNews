{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning goal\n",
    "In this notebook you will learn how to retrieve Google Trend's search interest data at scale and in a reliable way. This comes in handy for queries with thousands of keywords to build a dataset.  \n",
    "\n",
    "# Problem\n",
    "\n",
    "Using Google Trends to see search interest for a keyword of the last five years works well for a small number queries. With more queries, Google's server will deny their service, returning \"too many requests\" errors, rate limit exceedance, or blacklist your IP. A suggested workaround is to use proxies. However, this often causes other errors later on when they get out of service or have other issues which makes them inaccessible. You would have to reconfigure them once in a while when revisiting your code. \n",
    "\n",
    "To alleviate this problem, I rely on timeouts. Sufficiently long intervals between queries minimze the risk of request errors. In addition to this, I provide a fallback procedure in case of errors. It stores previously collected data and initiates another attempt. The procedure might take longer though especially for thousands of queries. But the computer can work, while you sleep. enjoy life or make big plans for the next project that involves Google Trends. Because you will know how to work it after reading this article.\n",
    "\n",
    "# Challenges and limits of Google Trends\n",
    "\n",
    "1. Denial of service, exceeding rate limits, being blacklisted\n",
    "3. Maximum 5 keywords per query\n",
    "2. Relative measures and scalability (solved by @Carrie Fowle\n",
    "in https://towardsdatascience.com/using-google-trends-at-scale-1c8b902b6bfa)\n",
    "\n",
    "\n",
    "While we focus on the first issue, there will be workarounds included for all three along the way.   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implementation plan\n",
    "\n",
    "\n",
    "\n",
    "1. look through code and judge where to refactor\n",
    "    1. rely on helper functions\n",
    "    2. define keyword_constructor()\n",
    "1. set Gtrends query into a function\n",
    "    2. save dataset along the way\n",
    "    2. simulate error and retry at last idx\n",
    "    3. if retry unsuccessful, increase timeout until no exception \n",
    "        1. abort with message if unsuccessful after 10 increases \"wait for a bit and define a longer timeout\"\n",
    "\n",
    "    \n",
    "## Data sources\n",
    "\n",
    "- search interest: Google Trends\n",
    "- search interest+: Google autocompletion\n",
    "- news coverage: Google News\n",
    "- ESG scores, financial data, sector: Yahoo!finance\n",
    "\n",
    "\n",
    "\n",
    "### NEXT\n",
    "* insert one scale word (popular search, i.e. COVID or corona) into batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6\n",
      "Pandas version 1.0.3\n"
     ]
    }
   ],
   "source": [
    "# data preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "from math import ceil\n",
    "from math import exp\n",
    "import os\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data collection \n",
    "from yahooquery import Ticker\n",
    "\n",
    "\n",
    "py_version = !python --version\n",
    "print(py_version[0])\n",
    "print(\"Pandas version\",pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inlcude into helper functions\n",
    "# KEYWORD GENERATOR HELPERS \n",
    "def regex_strip_legalname(raw_names):\n",
    "    \"\"\"Removes legal entity, technical description or firm type from firm name\n",
    "    \n",
    "    Input\n",
    "        raw_names: list of strings with firm names\n",
    "        \n",
    "    Return\n",
    "        list of strings: firm names without legal description \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pattern = r\"(,\\s)?(LLC|Inc|Corp\\w*|\\(?Class \\w+\\)?|Group|Company|\\WCo(\\s|\\.)|plc|Ltd|Int'l\\.|Holdings)\\.?\\W?\"\n",
    "    stripped_names = [re.sub(pattern,'', n) for n in raw_names]\n",
    "    \n",
    "    return stripped_names\n",
    "\n",
    "def batch(lst, n=5):\n",
    "    \"\"\"Yield successive n-sized chunks from list lst\n",
    "    \n",
    "    adapted from https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \n",
    "    Input\n",
    "        lst: list \n",
    "        n: selected batch size\n",
    "        \n",
    "    Return \n",
    "        List: lst divided into batches of len(lst)/n lists\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def flatten_list(nested_list):\n",
    "    \"\"\"Flattens nested list\"\"\"\n",
    "    \n",
    "    return [element for sublist in nested_list for element in sublist]\n",
    "\n",
    "def list_remove_duplicates(l):\n",
    "    \"\"\"Removes duplicates from list elements whilst preserving element order\n",
    "    adapted from \n",
    "    https://stackoverflow.com/questions/480214/how-do-you-remove-duplicates-from-a-list-whilst-preserving-order\n",
    "    \n",
    "    Input\n",
    "        list with string elements\n",
    "    \n",
    "    Return \n",
    "        Sorted list without duplicates\n",
    "    \n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in l if not (x in seen or seen_add(x))]\n",
    "\n",
    "    \n",
    "def make_x_y_csv(x, y, filename, data_dir):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param y: Data labels\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "    \n",
    "    # create dir if nonexistent\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # merge df\n",
    "    y = pd.DataFrame(y)\n",
    "    x = pd.DataFrame(x)\n",
    "    \n",
    "    # export to csv\n",
    "    pd.concat([y, x], axis=1).to_csv(os.path.join(data_dir, filename), \n",
    "                                     header=False, \n",
    "                                     index=False)\n",
    "    \n",
    "    # nothing is returned, but a print statement indicates that the function has run\n",
    "    print('Path created: '+str(data_dir)+'/'+str(filename))\n",
    "    \n",
    "def make_csv(x, filename, data_dir, append=False, header=False, index=False):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "    \n",
    "    # create dir if nonexistent\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # make sure its a df\n",
    "    x = pd.DataFrame(x)\n",
    "    \n",
    "    # export to csv\n",
    "    if not append:\n",
    "        x.to_csv(os.path.join(data_dir, filename), \n",
    "                                     header=header, \n",
    "                                     index=index)\n",
    "    else:\n",
    "        x.to_csv(os.path.join(data_dir, filename),\n",
    "                                     mode = 'a',\n",
    "                                     header=header, \n",
    "                                     index=index)        \n",
    "    \n",
    "    # nothing is returned, but a print statement indicates that the function has run\n",
    "    print('Path created: '+str(data_dir)+'/'+str(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineer search keywords from firm names and topic\n",
    "\n",
    "**TODO:** def construct_search_keywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve S&P 500 listings from Wikipedia\n",
    "table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "df_sp500 = table[0]\n",
    "\n",
    "## retrieve firm information from table\n",
    "# ticker\n",
    "ticker = list(df_sp500.Symbol)\n",
    "# sector\n",
    "sector = df_sp500.loc[:,'GICS Sector']\n",
    "# firm names\n",
    "firm_names_raw = list(df_sp500.Security)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocces firm names: Remove legal suffix\n",
    "\n",
    "TODO\n",
    "* remove class A/B\n",
    "* remove .com\n",
    "* apply list_remove_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes legal suffix from firm names\n",
    "from cleanco import prepare_terms, basename\n",
    "\n",
    "terms = prepare_terms()\n",
    "\n",
    "firm_names_clean = [basename(i, terms) for i in firm_names_raw]\n",
    "\n",
    "## previous solution with regex \n",
    "# remove legal taxonomy and firm type\n",
    "# TODO: add list_remove_duplicates()\n",
    "# firm_names = regex_strip_legalname(firm_names_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/topics.csv\n",
      "Path created: data/firm_names.csv\n"
     ]
    }
   ],
   "source": [
    "# esg keywords (negative exclusion criteria)\n",
    "topics = ['scandal', 'greenwashing', 'corruption', 'fraud', 'bribe', 'tax', 'forced', 'harassment', 'violation', \n",
    "          'human rights', 'conflict', 'weapons', 'arms trade', 'pollution', 'CO2', 'emission', 'fossil fuel',\n",
    "          'gender inequality', 'discrimination', 'sexism', 'racist', 'intransparent', 'data privacy', 'lawsuit', \n",
    "          'unfair', 'bad', 'problem', 'hate', 'issues', 'controversial']\n",
    "\n",
    "# store lists as csv for retrieval\n",
    "make_csv(topics, 'topics.csv', 'data', append=False, header=True)\n",
    "make_csv(firm_names, 'firm_names.csv', 'data', append=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> Subset for testing purposes\n",
      "Generated 900 keywords for 30 firms and 30 topics each\n",
      "Resulting in 180 queries with 5 keywords each (=batch)\n",
      "\n",
      "Example keyword batch:\n",
      "['scandal 3M', 'greenwashing 3M', 'corruption 3M', 'fraud 3M', 'bribe 3M']\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# DEFINE PARAMETERS \n",
    "n_firms = 30\n",
    "batch_size = 5\n",
    "n_keywords = int(n_firms*len(topics))\n",
    "n_query = int(n_keywords/batch_size)\n",
    "n_topics = len(topics)\n",
    "sec_sleep = 45\n",
    "############################\n",
    "\n",
    "\n",
    "# create search keywords as pairwise combintations of firm names + topics\n",
    "search_keywords = [[j+' '+i for j in topics] for i in firm_names]\n",
    "\n",
    "# print(\"{} topic keywords for {} firm each ---> {} pairwise combinations\"\\\n",
    "#       .format(n_topics, n_firms, n_keywords))\n",
    "# print()\n",
    "\n",
    "# Subset for test purposes\n",
    "print(\">>>>>>> Subset for testing purposes\")\n",
    "keywords_sample = search_keywords[:n_firms]\n",
    "print(\"Generated {} keywords for {} firms and {} topics each\".format(n_keywords,n_firms,n_topics))\n",
    "print(\"Resulting in {} queries with {} keywords each (=batch)\".format(n_query, batch_size))\n",
    "\n",
    "## generate keyword batches (= query)\n",
    "# flatten list\n",
    "keyword_batches = flatten_list([list(batch(keywords_sample[i], batch_size)) for i in range(n_firms)])\n",
    "\n",
    "print(\"\\nExample keyword batch:\\n{}\".format(keyword_batches[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Google\n",
    "\n",
    "* if error occurs, increase timeout, try again\n",
    "* temporarily save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_query_results(df_result, filename, keywords, data_dir='data', query_return_length=261):\n",
    "    \"\"\"Append query results to filename csv in data_dir directory\n",
    "        handle empty query results and build df containing 0s\n",
    "    \n",
    "    Input\n",
    "        df: dataframe containing query result (could be empty)\n",
    "        filename: name of temporary file\n",
    "        \n",
    "    \"\"\"\n",
    "    # non-empty df\n",
    "    if df_result.shape[0] != 0:\n",
    "        # append to csv\n",
    "        make_csv(df_result, filename=filename, data_dir=data_dir, append=True, header=True)\n",
    "\n",
    "\n",
    "    # empty df: no search result for any keyword\n",
    "    else:        \n",
    "        # create df containing 0s\n",
    "        # 261 is normal return length of query result (including \"isPartial\" column)\n",
    "        df_result = pd.DataFrame(np.zeros((query_return_length,batch_size)), columns=keywords)\n",
    "\n",
    "        # append to csv\n",
    "        make_csv(df_result, filename=filename, data_dir=data_dir, append=True, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/googletrends.csv\n"
     ]
    }
   ],
   "source": [
    "# create csv to store intermediate results\n",
    "temp_csv_name = 'googletrends.csv'\n",
    "make_csv(pd.DataFrame(),temp_csv_name, data_dir='data')\n",
    "\n",
    "def google_query(batched_keywords, temp_csv_name, sec_sleep=30):\n",
    "    \"\"\"Get Google trends data without interruptions\n",
    "        if server does not respond, retry with increased timeout\n",
    "        (i) \n",
    "    \n",
    "    Input\n",
    "        batched_keywords: list of keywords with chunks of five\n",
    "        temp_data: name of temporary file in ./data/ directory (defined in make_csv())\n",
    "    \n",
    "    \n",
    "    TODO\n",
    "        store csv in wide format\n",
    "        drop \"isPartial\" columns from df\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize pytrends\n",
    "    pt = TrendReq()\n",
    "    \n",
    "    ## iterate over keyword batches to obtain query results\n",
    "    for i, batch in enumerate(batched_keywords):\n",
    "    \n",
    "        # make query\n",
    "        try:\n",
    "            # pass keywords to pytrends API \n",
    "            pt.build_payload(kw_list=batch) \n",
    "\n",
    "            # store results from query in df\n",
    "            df_query_result = pt.interest_over_time()\n",
    "            \n",
    "            # store results\n",
    "            store_query_results(df_query_result, temp_csv_name, batch)\n",
    "            \n",
    "            # wait (timeout)\n",
    "            sleep(sec_sleep)\n",
    "        \n",
    "        # error handling\n",
    "        except Exception as e:\n",
    "            print(\"{} Query {} of {}\".format(e, i, n_query))\n",
    "            \n",
    "            # store index at which error occurred \n",
    "            # to restart whole function from that index\n",
    "            index_batch_error = i\n",
    "            \n",
    "            # recursively call function with keyword_batches starting from i\n",
    "            # and an increased timeout by 10 seconds\n",
    "            sec_sleep += 10\n",
    "            print(\"Increased sec_sleep to {}\".format(sec_sleep))\n",
    "            \n",
    "            google_query(batched_keywords[i:], sec_sleep+10)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['scandal 3M', 'greenwashing 3M', 'corruption 3M', 'fraud 3M', 'bribe 3M'],\n",
       " ['tax 3M', 'forced 3M', 'harassment 3M', 'violation 3M', 'human rights 3M']]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_batches[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/googletrends.csv\n",
      "Path created: data/googletrends.csv\n",
      "Path created: data/googletrends.csv\n",
      "Path created: data/googletrends.csv\n",
      "Path created: data/googletrends.csv\n",
      "Path created: data/googletrends.csv\n",
      "Path created: data/googletrends.csv\n",
      "Path created: data/googletrends.csv\n",
      "Path created: data/googletrends.csv\n",
      "Path created: data/googletrends.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-212-57efebe10b4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgoogle_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_csv_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msec_sleep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-211-a6148d2f9275>\u001b[0m in \u001b[0;36mgoogle_query\u001b[1;34m(batched_keywords, temp_csv_name, sec_sleep)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# wait (timeout)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msec_sleep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# error handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "google_query(keyword_batches[:2], temp_csv_name, sec_sleep=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrieve Google trends across time\n",
    "\n",
    "# initialize pytrends\n",
    "pt = TrendReq()\n",
    "\n",
    "# store DFs for later concat\n",
    "df_list = []\n",
    "index_batch_error = []\n",
    "\n",
    "# create csv to store intermediate results\n",
    "make_csv(pd.DataFrame(), 'googletrends.csv', data_dir='data')\n",
    "\n",
    "for i, batch in enumerate(keyword_batches):\n",
    "    \n",
    "    ## retrieve interest over time\n",
    "    try:\n",
    "        # init pytrends and wait (timeout)\n",
    "        pytrends_sleep_init(sec_sleep)\n",
    "        \n",
    "        # pass keywords to pytrends API\n",
    "        pt.build_payload(kw_list=batch) \n",
    "        \n",
    "        print(\"Payload build for {}. batch\".format(i))\n",
    "        df_search_result = pt.interest_over_time()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Query {} of {}\".format(i, n_query))\n",
    "        # store index at which error occurred\n",
    "        index_batch_error.append(i)\n",
    "        \n",
    "        # re-init pytrends and wait (sleep/timeout)\n",
    "        pytrends_sleep_init(sec_sleep)\n",
    "        \n",
    "        # retry\n",
    "        print(\"RETRY for {}. batch\".format(i))\n",
    "        pt.build_payload(kw_list=batch) \n",
    "        df_search_result = pt.interest_over_time()\n",
    "        \n",
    "    ## store query results\n",
    "    # check for non-empty df\n",
    "    if df_search_result.shape[0] != 0:\n",
    "        \n",
    "        # reset index for consistency (to call pd.concat later with empty dfs)\n",
    "        df_search_result.reset_index(inplace=True)\n",
    "        df_list.append(df_search_result)\n",
    "        \n",
    "    # no search result for any keyword\n",
    "    else:        \n",
    "        # create df containing 0s\n",
    "        df_search_result = pd.DataFrame(np.zeros((261,batch_size)), columns=batch)\n",
    "        df_list.append(df_search_result)\n",
    "        \n",
    "    make_csv(df_search_result, filename='googletrends.csv', data_dir='data',\n",
    "             append=True,\n",
    "            header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine query results to df\n",
    "drop_cols = ['isPartial', 'date']\n",
    "\n",
    "# index df\n",
    "df_clean_list = []\n",
    "for i,x in enumerate(range(0,len(df_list),6)):\n",
    "\n",
    "    map_colnames = dict(zip(search_keywords[i+272], list(topics)))\n",
    "    \n",
    "    ## create firm-level df\n",
    "    # df with isPartial and date columns --> drop columns\n",
    "    try:\n",
    "        df_firm = pd.concat(df_list[x:x+6], axis=1).drop(columns=drop_cols)\n",
    "\n",
    "        # rename columns\n",
    "        df_firm.rename(columns=map_colnames, inplace=True)\n",
    "        \n",
    "        # add firm column\n",
    "        df_firm['firm'] = firm_names[i+272]\n",
    "        \n",
    "        df_clean_list.append(df_firm)\n",
    "        \n",
    "    except:\n",
    "        df_firm = pd.concat(df_list[x:x+6], axis=1).rename(columns=map_colnames)\n",
    "        # rename columns\n",
    "        df_firm.rename(columns=map_colnames, inplace=True)\n",
    "        # add firm \n",
    "        df_firm['firm'] = firm_names[i]\n",
    "\n",
    "        df_clean_list.append(df_firm)\n",
    "\n",
    "# df (long format) with time dimension      \n",
    "df_time = pd.concat(df_clean_list)\n",
    "\n",
    "# Store query results so far\n",
    "print('Index batch error:',index_batch_error)\n",
    "\n",
    "# get timestamp\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df_filename = 'df_time_{}_idxbatch_{}.csv'.format(timestr, 1633)\n",
    "print(df_filename)\n",
    "\n",
    "# Store df_time\n",
    "make_csv(df_time, filename=df_filename, data_dir='data', append=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from GoogleNews import GoogleNews\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1\n",
      "Page: 2\n",
      "Page: 3\n",
      "Page: 4\n",
      "Page: 5\n",
      "(60, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>media</th>\n",
       "      <th>date</th>\n",
       "      <th>desc</th>\n",
       "      <th>link</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Microsoft pulls its smaller investments in fac...</td>\n",
       "      <td>Engadget</td>\n",
       "      <td>28.03.2020</td>\n",
       "      <td>Although Microsoft is less likely to be embroi...</td>\n",
       "      <td>https://www.engadget.com/2020-03-28-microsoft-...</td>\n",
       "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Bill Gates Gives to the Rich (Including Himself)</td>\n",
       "      <td>The Nation</td>\n",
       "      <td>17.03.2020</td>\n",
       "      <td>In speeches delivered at the American Enterpri...</td>\n",
       "      <td>https://www.thenation.com/article/society/bill...</td>\n",
       "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Coronavirus: Microsoft offers behind-the-scene...</td>\n",
       "      <td>ComputerWeekly.com</td>\n",
       "      <td>17.06.2020</td>\n",
       "      <td>ICO acknowledges GDPR concerns over A-level re...</td>\n",
       "      <td>https://www.computerweekly.com/news/252484794/...</td>\n",
       "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Microsoft fixes 26 critical vulnerabilities in...</td>\n",
       "      <td>ComputerWeekly.com</td>\n",
       "      <td>11.03.2020</td>\n",
       "      <td>Microsoft has fixed 115 CVE-numbered vulnerabi...</td>\n",
       "      <td>https://www.computerweekly.com/news/252479868/...</td>\n",
       "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Microsoft adds more AMD-powered Azure VMs, whi...</td>\n",
       "      <td>TechRepublic</td>\n",
       "      <td>07.11.2019</td>\n",
       "      <td>Microsoft adds more AMD-powered Azure VMs, whi...</td>\n",
       "      <td>https://www.techrepublic.com/article/microsoft...</td>\n",
       "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title               media  \\\n",
       "55  Microsoft pulls its smaller investments in fac...            Engadget   \n",
       "56   Bill Gates Gives to the Rich (Including Himself)          The Nation   \n",
       "57  Coronavirus: Microsoft offers behind-the-scene...  ComputerWeekly.com   \n",
       "58  Microsoft fixes 26 critical vulnerabilities in...  ComputerWeekly.com   \n",
       "59  Microsoft adds more AMD-powered Azure VMs, whi...        TechRepublic   \n",
       "\n",
       "          date                                               desc  \\\n",
       "55  28.03.2020  Although Microsoft is less likely to be embroi...   \n",
       "56  17.03.2020  In speeches delivered at the American Enterpri...   \n",
       "57  17.06.2020  ICO acknowledges GDPR concerns over A-level re...   \n",
       "58  11.03.2020  Microsoft has fixed 115 CVE-numbered vulnerabi...   \n",
       "59  07.11.2019  Microsoft adds more AMD-powered Azure VMs, whi...   \n",
       "\n",
       "                                                 link  \\\n",
       "55  https://www.engadget.com/2020-03-28-microsoft-...   \n",
       "56  https://www.thenation.com/article/society/bill...   \n",
       "57  https://www.computerweekly.com/news/252484794/...   \n",
       "58  https://www.computerweekly.com/news/252479868/...   \n",
       "59  https://www.techrepublic.com/article/microsoft...   \n",
       "\n",
       "                                                  img  \n",
       "55  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
       "56  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
       "57  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
       "58  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  \n",
       "59  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "googlenews=GoogleNews(start=date_1year_ago,end=date_today)\n",
    "googlenews.search('Microsoft scandal')\n",
    "\n",
    "until_page = 5\n",
    "for p in range(1, until_page+1):\n",
    "    print(\"Page:\", p)\n",
    "    googlenews.getpage(p)\n",
    "\n",
    "result = googlenews.result()\n",
    "df=pd.DataFrame(result)\n",
    "print(df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Microsoft Security Shocker As 250 Million Customer Records Exposed Online'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    50\n",
       "media    50\n",
       "date     50\n",
       "desc     50\n",
       "link     50\n",
       "img      50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from GoogleNews import GoogleNews\n",
    "from datetime import date\n",
    "\n",
    "# function to subtract a year form today's date\n",
    "def add_years(d, years):\n",
    "    \"\"\"Return a date that's `years` years after the date (or datetime)\n",
    "    object `d`. Return the same calendar date (month and day) in the\n",
    "    destination year, if it exists, otherwise use the following day\n",
    "    (thus changing February 29 to March 1).\n",
    "    \n",
    "    from https://stackoverflow.com/a/15743908\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return d.replace(year = d.year + years)\n",
    "    except ValueError:\n",
    "        return d + (date(d.year + years, 1, 1) - date(d.year, 1, 1))\n",
    "    \n",
    "\n",
    "\n",
    "def get_news(keyword, until_page=20):\n",
    "    \"\"\"Retrieve news for keyword for the first specified number of result pages\n",
    "        within the period until 1 year ago\n",
    "        \n",
    "    Input\n",
    "        keyword to look up news for\n",
    "    \n",
    "    Return\n",
    "        dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    ## define 1 year timespan with datestrings \n",
    "    # today's date\n",
    "    date_today = date.today().strftime(\"%m/%d/%Y\")\n",
    "    # date 1 year ago\n",
    "    date_1year_ago = add_years(date.today(), -1).strftime(\"%m/%d/%Y\")\n",
    "    \n",
    "    # init googlenews object\n",
    "    googlenews = GoogleNews(lang='en', start=,end='02/28/2020')\n",
    "    # search news for keyword\n",
    "    googlenews.search(keyword)\n",
    "    \n",
    "    # get results for each page \n",
    "    for p in range(until_page):\n",
    "        googlenews=GoogleNews(start=date_1year_ago,end=date_today)\n",
    "        googlenews.search('Microsoft scandal')\n",
    "        googlenews.getpage(1)\n",
    "        result = googlenews.result()\n",
    "        df=pd.DataFrame(result)\n",
    "        # print(df.head())\n",
    "        print(df.shape)\n",
    "        googlenews.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# LEGACY NOT NEEDED ANYMORE\n",
    "\n",
    "\n",
    "\n",
    "# todo: delete\n",
    "\n",
    "\n",
    "from pytrends.request import TrendReq \n",
    "from time import sleep \n",
    "\n",
    "# PYTREND HELPERS\n",
    "def pytrends_sleep_init(seconds, temp_csv):\n",
    "    \"\"\"Timeout for certain seconds and re-initialize pytrends\n",
    "    \n",
    "    Input\n",
    "        seconds: int with seconds for timeout\n",
    "        \n",
    "    Return\n",
    "        None\n",
    "    \n",
    "    \"\"\"\n",
    "    # print(\"TIMEOUT for {} sec.\".format(seconds))\n",
    "    sleep(seconds)\n",
    "    \n",
    "    # initialize pytrends\n",
    "    pt = TrendReq()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
